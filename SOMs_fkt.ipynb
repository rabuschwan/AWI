{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATLANTIC region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from netCDF4 import Dataset as NetCDFFile \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from scipy.fft import fft, fftfreq\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have already computed and stored your clusters in the 'clusters' variable\n",
    "\n",
    "# Convert clusters to a list of arrays (one array per cluster)\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "import minisom\n",
    "\n",
    "def einlesen(area, nc):\n",
    "    if area==\"ATLANTIC\":\n",
    "        #path_climatology = r\"N:/atm_glomod/user/jomuel001/CMIP6_models_prec/\"+model+\"/prec_ERA5_1985-2014.N_mjjaso_remapbnds.nc\"\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "        map = Basemap(projection='npstere', boundinglat=30, lon_0=0, llcrnrlon=-90, urcrnrlon=90)\n",
    "        #nc_climatology = NetCDFFile(path_climatology)\n",
    "        #print(nc)\n",
    "        #nc\n",
    "    \n",
    "        lat = nc.variables['lat'][:54]\n",
    "        lon = nc.variables['lon'][:]\n",
    "        reduced_lon = lon[:80]\n",
    "        lon = np.concatenate((reduced_lon, lon[-80:]), axis=0)\n",
    "        time = nc.variables['time'][:]\n",
    "        var = nc.variables['MSL'][:] # zonal wind\n",
    "        reduced_var = var[:, :54, :80]\n",
    "        var = np.concatenate((reduced_var, var[:, :54, -80:]), axis=2)\n",
    "        print(\"einlesen fertig\")\n",
    "        # Your data (var) and any preprocessing if needed\n",
    "        data = var\n",
    "        flattened_data = data.reshape(data.shape[0], -1)\n",
    "        data_normalized = (flattened_data - flattened_data.min()) / (flattened_data.max() - flattened_data.min())\n",
    "        return data_normalized, lon, lat, var\n",
    "    elif area==\"ARCTIC\":\n",
    "        #path_climatology = r\"N:/atm_glomod/user/jomuel001/CMIP6_models_prec/\"+model+\"/prec_ERA5_1985-2014.N_mjjaso_remapbnds.nc\"\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "        map = Basemap(projection='npstere', boundinglat=60, lon_0=0, llcrnrlon=-90, urcrnrlon=90)\n",
    "        #nc_climatology = NetCDFFile(path_climatology)\n",
    "        #print(nc)\n",
    "        #nc\n",
    "\n",
    "        lat = nc.variables['lat'][:28]\n",
    "        lon = nc.variables['lon'][:]\n",
    "\n",
    "        time = nc.variables['time'][:]\n",
    "        var = nc.variables['MSL'][:] # zonal wind\n",
    "        var = var[:, :28, :]\n",
    "        lr = LR  # Learning rate\n",
    "        sigma = SIGMA  # Sigma parameter\n",
    "        num_iter = NUM_ITER  # Number of iterations\n",
    "        print(\"einlesen fertig\")\n",
    "        # Your data (var) and any preprocessing if needed\n",
    "        data = var\n",
    "        flattened_data = data.reshape(data.shape[0], -1)\n",
    "        data_normalized = (flattened_data - flattened_data.min()) / (flattened_data.max() - flattened_data.min())\n",
    "        return data_normalized, lon, lat, var\n",
    "\n",
    "        \n",
    "        \n",
    "def clustering(LR, SIGMA, THRESHOLD,NUM_ITER, AREA, data_normalized, lon, lat, var,a,b, n_init, PLOTTEN):\n",
    "    area=AREA #ATLANTIC\n",
    "    method=\"analysis\" #analysis\n",
    "    number_cluster_soms=a*b\n",
    "    if area==\"ATLANTIC\":\n",
    "        print(\"First Training taking place\")\n",
    "        # Initialize the SOM\n",
    "        #num_iter = NUM_ITER\n",
    "        lr = LR\n",
    "        sigma = SIGMA\n",
    "        def updated_lr(lr,epoch,num_iter):\n",
    "            return lr\n",
    "        def updated_sigma(sigma,epoch, num_iter):\n",
    "            return sigma\n",
    "        \n",
    "        som = minisom.MiniSom(a, b, data_normalized.shape[1], sigma=sigma, learning_rate=lr, neighborhood_function=\"gaussian\")\n",
    "        \n",
    "        # Initialize weights randomly\n",
    "        for i in range(0,n_init,1):\n",
    "            som.random_weights_init(data_normalized)\n",
    "            num_iter = NUM_ITER\n",
    "            som.train(data_normalized, num_iteration=num_iter)\n",
    "            i+=1\n",
    "                # Plot the losses\n",
    "            # Training loop\n",
    "        print(f\"Number of epochs is {num_iter}\")\n",
    "        print(\"clustering findet statt\")\n",
    "        # Find the best-matching unit (BMU) for each data point\n",
    "        bmus = np.array([som.winner(d) for d in data_normalized])\n",
    "\n",
    "        # Create a dictionary to map BMUs to data points\n",
    "        bmu_to_data = {}\n",
    "        for i, bmu in enumerate(bmus):\n",
    "            bmu_tuple = tuple(bmu)  # Convert NumPy array to tuple\n",
    "            if bmu_tuple not in bmu_to_data:\n",
    "                bmu_to_data[bmu_tuple] = []\n",
    "            bmu_to_data[bmu_tuple].append(i)\n",
    "\n",
    "        # Convert the dictionary values (lists of data indices) to clusters\n",
    "        clusters = list(bmu_to_data.values())\n",
    "\n",
    "        # Print cluster sizes\n",
    "        sizes = [len(cluster) for cluster in clusters]\n",
    "        for i, cluster_size in enumerate(sizes):\n",
    "            normalized_size = cluster_size / 5520\n",
    "            print(f\"Cluster {i}: Size {normalized_size:.4f}\")\n",
    "        difference = max(sizes) - min(sizes)\n",
    "        print(difference)\n",
    "        print(\"SOMS fertig\")\n",
    "        \n",
    "        if method==\"analysis\":\n",
    "\n",
    "            summary = np.empty((0, 2), dtype=int)\n",
    "            import time \n",
    "\n",
    "            # Loop through each array and build the summary\n",
    "            clusters_to_process = [clusters[i] for i in range(number_cluster_soms)]  # This covers clusters[0] to clusters[8]\n",
    "\n",
    "            # Initialize the 'summary' array\n",
    "            summary = np.empty((0, 2), dtype=int)\n",
    "\n",
    "            # Loop through the selected clusters\n",
    "            for idx, arr in enumerate(clusters_to_process):\n",
    "                arr_summary = np.vstack((arr, np.full_like(arr, idx)))\n",
    "                summary = np.vstack((summary, arr_summary.T))\n",
    "\n",
    "            # Sort the summary array based on the first column (numbers)\n",
    "            sorted_summary = summary[summary[:, 0].argsort()]\n",
    "\n",
    "            np.savetxt(path+f\"AREA.-90_90_89.7849_29.0866/CLUSTER/SOMSslp_hpa_ERA5_1985-2014_mjjaso_atrbg_aacrm21_remapbnds_unnorm_{number_cluster_soms}tabcluster_10tabPC_dates.txt\", sorted_summary)\n",
    "            datesfile=path+f\"AREA.-90_90_89.7849_29.0866/CLUSTER/SOMSslp_hpa_ERA5_1985-2014_mjjaso_atrbg_aacrm21_remapbnds_unnorm_{number_cluster_soms}tabcluster_10tabPC_dates.txt\"\n",
    "        \n",
    "            dates=np.loadtxt(datesfile)\n",
    "            cluster=dates[:5520,1] # weil auch zukunft in dem ding drin ist\n",
    "            cluster_numbers = np.linspace(0,number_cluster_soms-1,number_cluster_soms)\n",
    "            iteration=0\n",
    "            while PLOTTEN and iteration==0:\n",
    "                print(\"PLOTTEN findet statt\")\n",
    "                FONTSIZE = 18\n",
    "                lons,lats = np.meshgrid(lon,lat)\n",
    "                # Loop through the cluster numbers\n",
    "                num_rows = a\n",
    "                num_cols = b\n",
    "                # Create a single figure to contain all the plots\n",
    "                fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5))  # Adjust figsize as needed\n",
    "\n",
    "                # Loop through the cluster numbers\n",
    "                for i, cluster_number in enumerate(cluster_numbers):\n",
    "                    map = Basemap(projection='npstere', boundinglat=30, lon_0=0, llcrnrlon=-90, urcrnrlon=90)\n",
    "                    x,y = map(lons,lats)\n",
    "                    nc = NetCDFFile(path + path_file)\n",
    "                    # Create a filter for the current cluster number\n",
    "                    cluster_filter = [x == cluster_number for x in cluster]\n",
    "                    FONTSIZE = 18\n",
    "                    # Calculate the mean for the current cluster\n",
    "                    soms = np.mean(var[cluster_filter, :, :], axis=0)\n",
    "\n",
    "                    # Define levels and boundaries\n",
    "                    levels = np.array([-15, -10, -7, -5, -3, -1, 1, 3, 5, 7, 10, 15])\n",
    "                    boundaries = np.linspace(-0, 0.1, 13)\n",
    "                    \n",
    "                    row_index = i // num_cols  # Integer division to determine row\n",
    "                    col_index = i % num_cols \n",
    "                    print(row_index, col_index)\n",
    "                    # Create the contour plot in the corresponding subplot\n",
    "                    # Create the contour plot in the corresponding subplot\n",
    "                    ax = axes[row_index, col_index] if num_rows > 1 else axes[col_index]\n",
    "                    variable = map.contourf(x, y, soms[:54, :], cmap=\"seismic\", levels=levels, zorder=5, extend='both', ax=ax)\n",
    "                    cb = map.colorbar(variable, ax=ax, fraction=0.05, pad=0.08, ticks=[-15, -10, -7, -5, -3, -1, 1, 3, 5, 7, 10, 15])\n",
    "\n",
    "                    # Customize the colorbar\n",
    "                    for t in cb.ax.get_xticklabels():\n",
    "                        t.set_fontsize(18)\n",
    "                    cb.set_ticklabels([\"-15\", \"-10\", \"-7\", \"-5\", \"-3\", \"-1\", \"1\", \"3\", \"5\", \"7\", \"10\", \"15\"])\n",
    "\n",
    "                    # Customize the plot title and labels\n",
    "                    if num_rows > 1:\n",
    "                        axes[row_index, col_index].set_title(f'SOMS Cluster {cluster_number}')\n",
    "                    else:\n",
    "                        axes[col_index].set_title(f'SOMS Cluster {cluster_number}')\n",
    "                    cb.set_label('slp [hPa]', fontsize=18)\n",
    "\n",
    "                    # Draw coastlines and countries\n",
    "                    ax = axes[row_index, col_index] if num_rows > 1 else axes[col_index]\n",
    "                    map.drawcoastlines(linewidth=0.3, zorder=6, ax=ax)\n",
    "                    map.drawcountries(linewidth=0.1, zorder=7, ax=ax)\n",
    "                for i in range(len(cluster_numbers), num_rows * num_cols):\n",
    "                    if num_rows > 1:\n",
    "                        fig.delaxes(axes[i // num_cols, i % num_cols])\n",
    "                    else:\n",
    "                        fig.delaxes(axes[i])\n",
    "                # Adjust spacing between subplots\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Save the figure\n",
    "                plt.savefig(f'N:/atm_glomod/user/jomuel001/CMIP6_models/ERA5/AREA.-90_90_89.7849_29.0866/CLUSTER/PLOTS/SOMS_slp_hpa_ERA5_1985-2014_mjjaso_atrbg_aacrm21_remabbnds_unnorm_{len(cluster_numbers)}cluster_10PC_{LR}LR_{SIGMA}Sigma_{num_iter}Epochs.png', dpi=300)\n",
    "\n",
    "                # Display the plot (optional)\n",
    "                plt.show()\n",
    "                iteration+=1\n",
    "        else:\n",
    "            print(\"pl\")\n",
    "    elif area==\"ARCTIC\":\n",
    "        print(\"ok\")\n",
    "    \n",
    "    pairwise_distances_matrix = pairwise_distances(data_normalized)\n",
    "    cluster_arrays = [data_normalized[cluster] for cluster in clusters]\n",
    "    # Calculate Silhouette Score using BMUs as cluster labels\n",
    "    silhouette_avg = silhouette_score(pairwise_distances_matrix, bmus.argmin(axis=1))\n",
    "    print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "\n",
    "    # Calculate Dunn Index\n",
    "    def dunn_index(cluster_arrays):\n",
    "        min_intercluster_distances = float('inf')\n",
    "        max_intracluster_diameter = 0.0\n",
    "\n",
    "        for cluster1 in cluster_arrays:\n",
    "            for cluster2 in cluster_arrays:\n",
    "                if cluster1 is not cluster2:\n",
    "                    # Calculate the minimum inter-cluster distance\n",
    "                    intercluster_distance = pairwise_distances(cluster1, cluster2, metric='euclidean').min()\n",
    "                    if intercluster_distance < min_intercluster_distances:\n",
    "                        min_intercluster_distances = intercluster_distance\n",
    "\n",
    "            # Calculate the maximum intra-cluster diameter\n",
    "            intracluster_diameter = euclidean(cluster1.max(axis=0), cluster1.min(axis=0))\n",
    "            if intracluster_diameter > max_intracluster_diameter:\n",
    "                max_intracluster_diameter = intracluster_diameter\n",
    "\n",
    "        return min_intercluster_distances / max_intracluster_diameter\n",
    "\n",
    "    dunn = dunn_index(cluster_arrays)\n",
    "    print(f\"Dunn Index: {dunn}\")\n",
    "    print(np.array([silhouette_avg, dunn]))\n",
    "    return silhouette_avg, dunn, bmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-3d7500e1d196>:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  lat = nc.variables['lat'][:54]\n",
      "<ipython-input-41-3d7500e1d196>:35: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  lon = nc.variables['lon'][:]\n",
      "<ipython-input-41-3d7500e1d196>:38: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  time = nc.variables['time'][:]\n",
      "<ipython-input-41-3d7500e1d196>:39: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  var = nc.variables['MSL'][:] # zonal wind\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "einlesen fertig\n",
      "First Training taking place\n"
     ]
    }
   ],
   "source": [
    "LR=np.array([0.07])\n",
    "SIGMA=np.array([0.2])\n",
    "THRESHOLD=1e-7\n",
    "NUM_ITER=np.array([1000])\n",
    "dunn=np.zeros(len(SIGMA))\n",
    "silhouette=np.zeros(len(SIGMA))\n",
    "AREA=\"ATLANTIC\"\n",
    "path = r\"N:/atm_glomod/user/jomuel001/CMIP6_models/ERA5/\"\n",
    "path_file = r\"slp_hpa_ERA5_1985-2014.N_mjjaso_atrbg_aacrm21_remapbnds.nc\"\n",
    "\n",
    "nc = NetCDFFile(path+path_file)\n",
    "data_normalized, lon, lat, var = einlesen(AREA, nc)\n",
    "\n",
    "\n",
    "silhouette, dunn, bmus = clustering(LR,SIGMA,THRESHOLD,NUM_ITER,AREA,data_normalized, lon, lat,var,1,5,n_init=100,PLOTTEN=True)\n",
    "print(f\"Der Silhouette score ist: {silhouette}\")\n",
    "print(f\"Der Dunn-score ist: {dunn}\")\n",
    "# plt.plot(SIGMA,dunn)\n",
    "# plt.savefig(f\"N:/atm_glomod/user/jomuel001/Dokumente/Presentations/ARBEIT/Dunn_score.png\")\n",
    "# plt.show()\n",
    "# plt.plot(SIGMA,silhouette)\n",
    "# plt.savefig(f\"N:/atm_glomod/user/jomuel001/Dokumente/Presentations/ARBEIT/Silhouette_score.png\")\n",
    "# plt.show()\n",
    "# Assuming 'bmus' is your list of BMUs\n",
    "#Kmeans(bmus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [2, 0],\n",
       "       [0, 0],\n",
       "       ...,\n",
       "       [2, 1],\n",
       "       [2, 1],\n",
       "       [2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "def Kmeans(bmus):\n",
    "    # Assuming 'bmus' is your list of BMUs\n",
    "    num_clusters = 5  # Number of clusters\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "    kmeans.fit(bmus)\n",
    "\n",
    "    # Get the cluster labels assigned by K-means\n",
    "    kmeans_labels = kmeans.labels_\n",
    "    np.savetxt(path+r\"AREA.-90_90_89.7849_29.0866/CLUSTER/SOMSKMEANSslp_hpa_ERA5_1985-2014_mjjaso_atrbg_aacrm21_remapbnds_unnorm_5tabcluster_10tabPC_dates.txt\", kmeans_labels)\n",
    "    datesfile=path+r\"AREA.-90_90_89.7849_29.0866/CLUSTER/SOMSKMEANSslp_hpa_ERA5_1985-2014_mjjaso_atrbg_aacrm21_remapbnds_unnorm_5tabcluster_10tabPC_dates.txt\"\n",
    "    FONTSIZE = 18\n",
    "    lons,lats = np.meshgrid(lon,lat)\n",
    "    dates=np.loadtxt(datesfile)\n",
    "    cluster=dates[:5520] # weil auch zukunft in dem ding drin ist\n",
    "    cluster_numbers = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "    iteration=0\n",
    "    fig, axes = plt.subplots(1, len(cluster_numbers), figsize=(16, 4))  # Adjust figsize as needed\n",
    "    # Assuming kmeans_labels contains the cluster assignments\n",
    "    cluster_counts = Counter(kmeans_labels)\n",
    "\n",
    "    # Print the frequency of occurrence for each cluster\n",
    "    for cluster_num, count in cluster_counts.items():\n",
    "        print(f'Cluster {cluster_num}: Frequency = {count/5520}')\n",
    "    # Loop through the cluster numbers\n",
    "    for i, cluster_number in enumerate(cluster_numbers):\n",
    "        map = Basemap(projection='npstere', boundinglat=30, lon_0=0, llcrnrlon=-90, urcrnrlon=90)\n",
    "        x,y = map(lons,lats)\n",
    "        nc = NetCDFFile(path + path_file)\n",
    "        # Create a filter for the current cluster number\n",
    "        cluster_filter = [x == cluster_number for x in cluster]\n",
    "        FONTSIZE = 18\n",
    "        # Calculate the mean for the current cluster\n",
    "        soms = np.mean(var[cluster_filter, :, :], axis=0)\n",
    "\n",
    "        # Define levels and boundaries\n",
    "        levels = np.array([-15, -10, -7, -5, -3, -1, 1, 3, 5, 7, 10, 15])\n",
    "        boundaries = np.linspace(-0, 0.1, 13)\n",
    "\n",
    "        # Create the contour plot in the corresponding subplot\n",
    "        variable = map.contourf(x, y, soms[:54, :], cmap=\"seismic\", levels=levels, zorder=5, extend='both', ax=axes[i])\n",
    "        cb = map.colorbar(variable, ax=axes[i],fraction=0.05, pad=0.08,\n",
    "                          ticks=[-15, -10, -7, -5, -3, -1, 1, 3, 5, 7, 10, 15])\n",
    "\n",
    "        # Customize the colorbar\n",
    "        for t in cb.ax.get_xticklabels():\n",
    "            t.set_fontsize(18)\n",
    "        cb.set_ticklabels([\"-15\", \"-10\", \"-7\", \"-5\", \"-3\", \"-1\", \"1\", \"3\", \"5\", \"7\", \"10\", \"15\"])\n",
    "\n",
    "        # Customize the plot title and labels\n",
    "        axes[i].set_title(f'KMEANS Cluster {cluster_number}')\n",
    "        cb.set_label('slp [hPa]', fontsize=18)\n",
    "\n",
    "        # Draw coastlines and countries\n",
    "        map.drawcoastlines(linewidth=0.3, zorder=6, ax=axes[i])\n",
    "        map.drawcountries(linewidth=0.1, zorder=7, ax=axes[i])\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    # Save the figure\n",
    "\n",
    "    # Save the figure for the current cluster\n",
    "    plt.savefig(f'N:/atm_glomod/user/jomuel001/CMIP6_models/ERA5/AREA.-90_90_89.7849_29.0866/CLUSTER/PLOTS/SOMSKMEANS_slp_hpa_ERA5_1985-2014_mjjaso_atrbg_aacrm21_remabbnds_unnorm_{len(cluster_numbers)}cluster_10PC_{LR}LR_{SIGMA}Sigma.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
